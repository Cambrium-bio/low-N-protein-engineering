{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this notebook is computationally intensive. You'll need to be running the GPU docker. Additionally, ensure you have plenty of disk space, and ideally, multiple CPUs available. Intermediate results have been stored and are accessible on S3. See the Supp_Fig_2.ipynb notebook for accessing and plotting this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "\n",
    "sys.path.append('../common')\n",
    "import data_io_utils\n",
    "import paths\n",
    "import utils\n",
    "import constants\n",
    "\n",
    "import A003_common\n",
    "import policy_evaluation\n",
    "import acquisition_policies\n",
    "import models\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_io_utils.sync_s3_path_to_local(paths.DATASETS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_io_utils.sync_s3_path_to_local(paths.POLICY_EVAL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_io_utils.sync_s3_path_to_local(paths.EVOTUNING_CKPT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(8329)\n",
    "np.random.seed(4158)\n",
    "\n",
    "FORCE = True\n",
    "\n",
    "N_REPLICATES = 20\n",
    "SPLIT = 2 # Split 0 used for training, 1 for prospective design, 2 for final figure.\n",
    "\n",
    "training_sets = ['sarkisyan']\n",
    "acq_policies = ['random']\n",
    "n_training_points_schedule = np.array([8, 24, 96])\n",
    "\n",
    "models = [\n",
    "    'LassoLars',\n",
    "    'Ridge',\n",
    "    'RidgeSparseRefit',\n",
    "    'EnsembledRidgeSparseRefit',\n",
    "    \n",
    "    'Doc2VecLassoLars',\n",
    "    'Doc2VecRidge',\n",
    "    'Doc2VecRidgeSparseRefit',\n",
    "    'Doc2VecEnsembledRidgeSparseRefit',\n",
    "    \n",
    "    'UniRepLassoLars', \n",
    "    'UniRepRidge',\n",
    "    'UniRepRidgeSparseRefit',\n",
    "    'UniRepEnsembledRidgeSparseRefit',\n",
    "    \n",
    "    'EvotunedUniRep_Random_Init_1_LassoLars', \n",
    "    'EvotunedUniRep_Random_Init_1_Ridge',\n",
    "    'EvotunedUniRep_Random_Init_1_RidgeSparseRefit',\n",
    "    'EvotunedUniRep_Random_Init_1_EnsembledRidgeSparseRefit',\n",
    "    \n",
    "    'EvotunedUniRep_Global_Init_1_LassoLars', \n",
    "    'EvotunedUniRep_Global_Init_1_Ridge',\n",
    "    'EvotunedUniRep_Global_Init_1_RidgeSparseRefit',\n",
    "    'EvotunedUniRep_Global_Init_1_EnsembledRidgeSparseRefit',\n",
    "    \n",
    "    'EvotunedUniRep_Global_Init_2_LassoLars', \n",
    "    'EvotunedUniRep_Global_Init_2_Ridge',\n",
    "    'EvotunedUniRep_Global_Init_2_RidgeSparseRefit',\n",
    "    'EvotunedUniRep_Global_Init_2_EnsembledRidgeSparseRefit',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP homolog parents contained in gen set: 3\n",
      "CHANGED N_TRAINING_POINTS_SCHEDULE\n",
      "/notebooks/analysis/common/../../data/s3/policy_evaluation/split_2/train_sarkisyan/LassoLarsModel/RandomAcquisition\n",
      "No previous data found on S3.\n",
      "Found no existing progress or being forced.\n",
      "n_train: 8\n",
      "\tAcquiring training points\n",
      "\tTraining/tuning model\n",
      "\tEvaluating generalization\n",
      "\tCheckpointing\n",
      "n_train: 24\n",
      "\tAcquiring training points\n",
      "\tTraining/tuning model\n",
      "\tEvaluating generalization\n",
      "\tCheckpointing\n",
      "n_train: 96\n",
      "\tAcquiring training points\n",
      "\tTraining/tuning model\n",
      "\tEvaluating generalization\n",
      "\tCheckpointing\n",
      "Found no existing progress or being forced.\n",
      "n_train: 8\n",
      "\tAcquiring training points\n",
      "\tTraining/tuning model\n",
      "\tEvaluating generalization\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    for acq_policy in acq_policies:\n",
    "        for training_set in training_sets:\n",
    "            \n",
    "            ## Load inputs\n",
    "            inputs = policy_evaluation.load_data_eff_inputs(\n",
    "                split=SPLIT, \n",
    "                training_set_name=training_set, \n",
    "                acq_policy=acq_policy, \n",
    "                model=model)\n",
    "            \n",
    "            print('CHANGED N_TRAINING_POINTS_SCHEDULE')\n",
    "            inputs['n_training_points_schedule'] = n_training_points_schedule\n",
    "            \n",
    "            ## Sync any previous progress from S3\n",
    "            print(inputs['root_output_dir'])\n",
    "            if data_io_utils.path_exists_on_s3(inputs['root_output_dir']):\n",
    "                print('Found previous data on S3.')\n",
    "                data_io_utils.sync_s3_path_to_local(inputs['root_output_dir'])\n",
    "            else:\n",
    "                print('No previous data found on S3.') \n",
    "            \n",
    "            ## RUN\n",
    "            for i in range(N_REPLICATES):\n",
    "                results = policy_evaluation.evaluate_model_and_acquisition_policy(\n",
    "                    inputs['training_set_df'],\n",
    "                    inputs['acquisition_policy_obj'],\n",
    "                    inputs['n_training_points_schedule'],\n",
    "                    inputs['acquisition_policy_params'],\n",
    "                    inputs['model_obj'],\n",
    "                    inputs['generalization_set_dfs'],\n",
    "                    inputs['generalization_set_names'],\n",
    "                    inputs['generalization_set_sub_category_columns'],\n",
    "                    inputs['generalization_set_calc_params'],\n",
    "                    os.path.join(inputs['root_output_dir'], 'rep_' + str(i)), # subdir for replicate\n",
    "                    force=FORCE,\n",
    "                    verbose=True,\n",
    "                    save_models=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sync to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_io_utils.sync_local_path_to_s3(paths.POLICY_EVAL_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
